[paths]
train = null
dev = null
vectors = null
init_tok2vec = null
ner_source = null
parser_source = null

[system]
seed = 0
gpu_allocator = null

[nlp]
lang = "en"
pipeline = ["tok2vec","ner","parser","tagger"]
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
batch_size = 1000
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}

[components]

[components.tok2vec]
source = ${paths.ner_source}
component = "tok2vec"

[components.ner]
source = ${paths.ner_source}
component = "ner"

[components.parser]
source = ${paths.parser_source}
component = "parser"

[components.tagger]
source = ${paths.parser_source}
component = "tagger"

[components.tagger.model]
@architectures = "spacy.Tagger.v1"
nO = null

[components.tagger.model.tok2vec]
@architectures = "spacy.Tok2Vec.v2"

[components.tagger.model.tok2vec.embed]
@architectures = "spacy.MultiHashEmbed.v2"
width = 96
attrs = ["NORM","PREFIX","SUFFIX","SHAPE","DEP"]
rows = [5000,2500,2500,2500,2500]
include_static_vectors = false

[components.tagger.model.tok2vec.encode]
@architectures = "spacy.MaxoutWindowEncoder.v2"
width = 96
depth = 4
window_size = 1
maxout_pieces = 3


[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]